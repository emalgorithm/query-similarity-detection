{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from text_processor import TextProcessor\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import gensim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from util import get_balanced_data\n",
    "from word2vec import Word2Vec\n",
    "from adversarial_algos import adversarial_white_box_change\n",
    "from word_in_common_feature_extractor import WordsInCommonFeatureExtractor\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "pal = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 298526 positive samples\n",
      "We have 298526 negative samples\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_balanced_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WordsInCommonFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.get_features_for_sample(\"hello\", \"hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# def words_in_common_similarity(q1, q2):\n",
    "#     \"\"\"\n",
    "#     The similarity score is the ratio of words that appear in both questions over words which appear in either.\n",
    "#     Stop words are excluded.\n",
    "#     \"\"\"\n",
    "#     q1_words = set([w for w in tp.tokenize(q1.lower()) if w not in stopwords])\n",
    "#     q2_words = set([w for w in tp.tokenize(q2.lower()) if w not in stopwords])\n",
    "    \n",
    "#     words_in_common = len(q1_words & q2_words)\n",
    "#     total_words = len(q1_words | q2_words) \n",
    "    \n",
    "#     frac_words_in_common = words_in_common / total_words\n",
    "    \n",
    "#     return frac_words_in_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = feature_extractor.get_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 482\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num = feature_extractor.get_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-973bbfa65af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coef_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bin = [0 if y < 0.38 else 1 for y in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X):\n",
    "    return np.apply_along_axis(lambda x: words_in_common_similarity(x[0], x[1]), 1, X).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41 s, sys: 3.26 s, total: 44.2 s\n",
      "Wall time: 44.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = Word2Vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial White Box Algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(q1, q2):\n",
    "    x = np.array([q1, q2]).reshape(1, -1)\n",
    "    x_num = get_features(x)\n",
    "    \n",
    "    return clf.predict(x_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adversarial_change(q1, q2, model):\n",
    "#     \"\"\"\n",
    "#     Initially, q1 and q2 are detected as similar by the classifier.\n",
    "#     Change q2 so that it retains the same meaning and is now detected as not similar to q2\n",
    "#     \"\"\"    \n",
    "#     q1_tokenized = tp.tokenize(q1)\n",
    "#     q2_tokenized = tp.tokenize(q2)\n",
    "#     successful = False\n",
    "#     replaced_words = 0\n",
    "#     print(\"initial q1: {}\".format(q1))\n",
    "#     print(\"initial q2: {}\".format(q2))\n",
    "#     print(\"Initial similarity is {}\".format(model(q1_tokenized, q2_tokenized, clf)))\n",
    "    \n",
    "#     while not successful or replaced_words >= 5:\n",
    "#         # Try changing each word in q2. At the end, select the change that gives the best improvement\n",
    "#         min_score = model(q1_tokenized, q2_tokenized, clf)\n",
    "#         new_q2 = q2_tokenized\n",
    "        \n",
    "#         for i, word in enumerate(q2_tokenized):\n",
    "#             if word in w2v.model.vocab:\n",
    "#                 closest_word = w2v.get_closest_word(word)\n",
    "#                 q2_modified = list(q2_tokenized)\n",
    "#                 q2_modified[i] = closest_word\n",
    "#                 score = model(q1_tokenized, q2_modified, clf)\n",
    "\n",
    "#                 if score < min_score:\n",
    "#                     min_score = score\n",
    "#                     new_q2 = q2_modified\n",
    "        \n",
    "#         if min_score < model(q1_tokenized, q2_tokenized, clf):\n",
    "#             q2_tokenized = new_q2\n",
    "#             replaced_words += 1\n",
    "        \n",
    "#         print()\n",
    "#         print(\"q1: '{}'\".format(tp.detokenize(q1_tokenized)))\n",
    "#         print(\"q2: '{}'\".format(tp.detokenize(new_q2)))\n",
    "#         print(\"Replacing {} words we have a similarity of {}\".format(replaced_words, min_score))\n",
    "        \n",
    "#         if min_score < 0.4:\n",
    "#             successful = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial q1: Who is the richest gambler of all time and how can I reach his level?\n",
      "initial q2: Who is the richest gambler of all time and how can I reach his level as a gambler?\n",
      "Initial similarity is [0.94920687]\n",
      "\n",
      "q1: 'Who is the richest gambler of all time and how can I reach his level?'\n",
      "q2: 'Who is the wealthiest gambler of all time and how can I reach his level as a gambler?'\n",
      "Replacing 1 words we have a similarity of [0.70803868]\n",
      "\n",
      "q1: 'Who is the richest gambler of all time and how can I reach his level?'\n",
      "q2: 'Who is the wealthiest gambler of all day and how can I reach his level as a gambler?'\n",
      "Replacing 2 words we have a similarity of [0.52716254]\n",
      "\n",
      "q1: 'Who is the richest gambler of all time and how can I reach his level?'\n",
      "q2: 'Who is the wealthiest gambler of all day and how can I reaching his level as a gambler?'\n",
      "Replacing 3 words we have a similarity of [0.38648109]\n"
     ]
    }
   ],
   "source": [
    "q1 = 'Who is the richest gambler of all time and how can I reach his level?'\n",
    "q2 = 'Who is the richest gambler of all time and how can I reach his level as a gambler?'\n",
    "\n",
    "adversarial_white_box_change(q1, q2, model, tp, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46687049])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tp.tokenize('Who is the  of n I reach his level?'), tp.tokenize('Who is the richest gambler of all time and how can I reach his level as a gambler?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial q1: Can an introvert become an extrovert?\n",
      "initial q2: Can an introvert and an extrovert be together?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'Can an introvert become an extrovert?'\n",
      "q2: 'Can an introverted and an extrovert be together?'\n",
      "Replacing 1 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: Is \"Out\" based on real events?\n",
      "initial q2: Is \"G\" based on real events?\n",
      "Initial similarity is [0.82862278]\n",
      "\n",
      "q1: 'Is``Out\"based on real events?'\n",
      "q2: 'Is``G\"based on genuine events?'\n",
      "Replacing 1 words we have a similarity of [0.63267362]\n",
      "\n",
      "q1: 'Is``Out\"based on real events?'\n",
      "q2: 'Is``G\"based on genuine event?'\n",
      "Replacing 2 words we have a similarity of [0.48026872]\n",
      "\n",
      "q1: 'Is``Out\"based on real events?'\n",
      "q2: 'Is``##m_Tc_EC\"based on genuine event?'\n",
      "Replacing 3 words we have a similarity of [0.44275367]\n",
      "\n",
      "q1: 'Is``Out\"based on real events?'\n",
      "q2: 'Is``##m_Tc_EC\"based onthe genuine event?'\n",
      "Replacing 4 words we have a similarity of [0.41205954]\n",
      "UNSUCCESSFULL\n",
      "\n",
      "initial q1: How hair grow after hair transplant surgery?\n",
      "initial q2: Where does the hair come from in a hair transplant surgery?\n",
      "Initial similarity is [0.66784398]\n",
      "\n",
      "q1: 'How hair grow after hair transplant surgery?'\n",
      "q2: 'Where does the hair come from in a hair transplants surgery?'\n",
      "Replacing 1 words we have a similarity of [0.46687049]\n",
      "\n",
      "q1: 'How hair grow after hair transplant surgery?'\n",
      "q2: 'Where does the hair come from in a hair transplants surgeries?'\n",
      "Replacing 2 words we have a similarity of [0.31614037]\n",
      "\n",
      "initial q1: How do I get a scholarship to study in abroad?\n",
      "initial q2: How do I get a scholarship to study abroad?\n",
      "Initial similarity is [0.94920687]\n",
      "\n",
      "q1: 'How do I get a scholarship to study in abroad?'\n",
      "q2: 'How do I getting a scholarship to study abroad?'\n",
      "Replacing 1 words we have a similarity of [0.66784398]\n",
      "\n",
      "q1: 'How do I get a scholarship to study in abroad?'\n",
      "q2: 'How do I getting a scholarships to study abroad?'\n",
      "Replacing 2 words we have a similarity of [0.46687049]\n",
      "\n",
      "q1: 'How do I get a scholarship to study in abroad?'\n",
      "q2: 'How do I getting a scholarships to studies abroad?'\n",
      "Replacing 3 words we have a similarity of [0.31614037]\n",
      "\n",
      "initial q1: Do you think chatbots will take off?\n",
      "initial q2: Are chatbots going to take off?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'Do you think chatbots will take off?'\n",
      "q2: 'Are chatbot going to take off?'\n",
      "Replacing 1 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: What individuals and events in history are a source of pride for Algeria?\n",
      "initial q2: What individuals and events in history are a source of pride for China?\n",
      "Initial similarity is [0.73818471]\n",
      "\n",
      "q1: 'What individuals and events in history are a source of pride for Algeria?'\n",
      "q2: 'What persons and events in history are a source of pride for China?'\n",
      "Replacing 1 words we have a similarity of [0.57405635]\n",
      "\n",
      "q1: 'What individuals and events in history are a source of pride for Algeria?'\n",
      "q2: 'What persons and event in history are a source of pride for China?'\n",
      "Replacing 2 words we have a similarity of [0.44275367]\n",
      "\n",
      "q1: 'What individuals and events in history are a source of pride for Algeria?'\n",
      "q2: 'What persons and event in annals are a source of pride for China?'\n",
      "Replacing 3 words we have a similarity of [0.3353242]\n",
      "\n",
      "initial q1: Who is Rian Johnson?\n",
      "initial q2: What is Rian Johnson working on?\n",
      "Initial similarity is [0.73818471]\n",
      "\n",
      "q1: 'Who is Rian Johnson?'\n",
      "q2: 'What is Lynlee Johnson working on?'\n",
      "Replacing 1 words we have a similarity of [0.44275367]\n",
      "\n",
      "q1: 'Who is Rian Johnson?'\n",
      "q2: 'What is Lynlee Smith working on?'\n",
      "Replacing 2 words we have a similarity of [0.24579965]\n",
      "\n",
      "initial q1: How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Great Victoria Desert?\n",
      "initial q2: How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Karakum Desert?\n",
      "Initial similarity is [0.75441718]\n",
      "\n",
      "q1: 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Great Victoria Desert?'\n",
      "q2: 'How cold can the Digi_TransPort_WR## Desert get, and how do its average temperatures compare to the ones in the Karakum Desert?'\n",
      "Replacing 1 words we have a similarity of [0.6115714]\n",
      "\n",
      "q1: 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Great Victoria Desert?'\n",
      "q2: 'How chilly can the Digi_TransPort_WR## Desert get, and how do its average temperatures compare to the ones in the Karakum Desert?'\n",
      "Replacing 2 words we have a similarity of [0.52716254]\n",
      "\n",
      "q1: 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Great Victoria Desert?'\n",
      "q2: 'How chilly can the Digi_TransPort_WR## Desert getting, and how do its average temperatures compare to the ones in the Karakum Desert?'\n",
      "Replacing 3 words we have a similarity of [0.45268412]\n",
      "\n",
      "q1: 'How cold can the Gobi Desert get, and how do its average temperatures compare to the ones in the Great Victoria Desert?'\n",
      "q2: 'How chilly can the Digi_TransPort_WR## Desert getting, and how do its averages temperatures compare to the ones in the Karakum Desert?'\n",
      "Replacing 4 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: What is the best part time job to work from home?\n",
      "initial q2: What are the ways to earn in a part time job work from home?\n",
      "Initial similarity is [0.66784398]\n",
      "\n",
      "q1: 'What is the best part time job to work from home?'\n",
      "q2: 'What are the ways to earn in a integral_part time job work from home?'\n",
      "Replacing 1 words we have a similarity of [0.52716254]\n",
      "\n",
      "q1: 'What is the best part time job to work from home?'\n",
      "q2: 'What are the ways to earn in a integral_part day job work from home?'\n",
      "Replacing 2 words we have a similarity of [0.41205954]\n",
      "\n",
      "q1: 'What is the best part time job to work from home?'\n",
      "q2: 'What are the ways to earn in a integral_part day jobs work from home?'\n",
      "Replacing 3 words we have a similarity of [0.31614037]\n",
      "\n",
      "initial q1: What are the best cars for beginners?\n",
      "initial q2: Which car is the best for beginners?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'What are the best cars for beginners?'\n",
      "q2: 'Which car is the finest for beginners?'\n",
      "Replacing 1 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: How many air craft carriers does India have, and are they new or refurbished?\n",
      "initial q2: How many air craft carriers do India presently have?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'How many air craft carriers does India have, and are they new or refurbished?'\n",
      "q2: 'How several air craft carriers do India presently have?'\n",
      "Replacing 1 words we have a similarity of [0.48879487]\n",
      "\n",
      "q1: 'How many air craft carriers does India have, and are they new or refurbished?'\n",
      "q2: 'How several sulfurous_smell_wafted craft carriers do India presently have?'\n",
      "Replacing 2 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: Will war happen between India and Pakistan?\n",
      "initial q2: Will there be a nuclear war between India and Pakistan?\n",
      "Initial similarity is [0.66784398]\n",
      "\n",
      "q1: 'Will war happen between India and Pakistan?'\n",
      "q2: 'Will there be a nuclear wars between India and Pakistan?'\n",
      "Replacing 1 words we have a similarity of [0.46687049]\n",
      "\n",
      "q1: 'Will war happen between India and Pakistan?'\n",
      "q2: 'Will there be a nuclear wars between Indias and Pakistan?'\n",
      "Replacing 2 words we have a similarity of [0.31614037]\n",
      "\n",
      "initial q1: Which are the best SIP to invest in India?\n",
      "initial q2: What is the best SIP Plan in India?\n",
      "Initial similarity is [0.66784398]\n",
      "\n",
      "q1: 'Which are the best SIP to invest in India?'\n",
      "q2: 'What is the finest SIP Plan in India?'\n",
      "Replacing 1 words we have a similarity of [0.46687049]\n",
      "\n",
      "q1: 'Which are the best SIP to invest in India?'\n",
      "q2: 'What is the finest Session_Initiation_Protocol_SIP Plan in India?'\n",
      "Replacing 2 words we have a similarity of [0.31614037]\n",
      "\n",
      "initial q1: What are the best horror movies on Netflix right now?\n",
      "initial q2: What are the best horror movies on Netflix?\n",
      "Initial similarity is [0.80852543]\n",
      "\n",
      "q1: 'What are the best horror movies on Netflix right now?'\n",
      "q2: 'What are the finest horror movies on Netflix?'\n",
      "Replacing 1 words we have a similarity of [0.58745459]\n",
      "\n",
      "q1: 'What are the best horror movies on Netflix right now?'\n",
      "q2: 'What are the finest horror films on Netflix?'\n",
      "Replacing 2 words we have a similarity of [0.42165145]\n",
      "\n",
      "q1: 'What are the best horror movies on Netflix right now?'\n",
      "q2: 'What are the finest horror films onthe Netflix?'\n",
      "Replacing 3 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: What is the meaning of Rx? Which is written on the the doctor's Slip?\n",
      "initial q2: What is the meaning of Rx which is written on the the doctor's Slip?\n",
      "Initial similarity is [0.94920687]\n",
      "\n",
      "q1: 'What is the meaning of Rx? Which is written on the the doctor's Slip?'\n",
      "q2: 'What is the meanings of Rx which is written on the the doctor's Slip?'\n",
      "Replacing 1 words we have a similarity of [0.73818471]\n",
      "\n",
      "q1: 'What is the meaning of Rx? Which is written on the the doctor's Slip?'\n",
      "q2: 'What is the meanings of Prescription which is written on the the doctor's Slip?'\n",
      "Replacing 2 words we have a similarity of [0.57405635]\n",
      "\n",
      "q1: 'What is the meaning of Rx? Which is written on the the doctor's Slip?'\n",
      "q2: 'What is the meanings of Prescription which is penned on the the doctor's Slip?'\n",
      "Replacing 3 words we have a similarity of [0.44275367]\n",
      "\n",
      "q1: 'What is the meaning of Rx? Which is written on the the doctor's Slip?'\n",
      "q2: 'What is the meanings of Prescription which is penned on the the physician's Slip?'\n",
      "Replacing 4 words we have a similarity of [0.3353242]\n",
      "\n",
      "initial q1: What are the jobs that involve a bit of adventure?\n",
      "initial q2: What are some jobs that involve adventure?\n",
      "Initial similarity is [0.78038914]\n",
      "\n",
      "q1: 'What are the jobs that involve a bit of adventure?'\n",
      "q2: 'What are some employment that involve adventure?'\n",
      "Replacing 1 words we have a similarity of [0.52716254]\n",
      "\n",
      "q1: 'What are the jobs that involve a bit of adventure?'\n",
      "q2: 'What are some employment that entail adventure?'\n",
      "Replacing 2 words we have a similarity of [0.34628639]\n",
      "\n",
      "initial q1: Can first love last?\n",
      "initial q2: Can first loves last?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'Can first love last?'\n",
      "q2: 'Can second loves last?'\n",
      "Replacing 1 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: What will be your verdict on the Cauvery river water dispute between Karnataka and Tamil Nadu?\n",
      "initial q2: What is your say on the Karnataka Tamil Nadu Cauvery water dispute?\n",
      "Initial similarity is [0.69598027]\n",
      "\n",
      "q1: 'What will be your verdict on the Cauvery river water dispute between Karnataka and Tamil Nadu?'\n",
      "q2: 'What is your say on the Andhra_Pradesh Tamil Nadu Cauvery water dispute?'\n",
      "Replacing 1 words we have a similarity of [0.5655302]\n",
      "\n",
      "q1: 'What will be your verdict on the Cauvery river water dispute between Karnataka and Tamil Nadu?'\n",
      "q2: 'What is your say on the Andhra_Pradesh Tamils Nadu Cauvery water dispute?'\n",
      "Replacing 2 words we have a similarity of [0.45682181]\n",
      "\n",
      "q1: 'What will be your verdict on the Cauvery river water dispute between Karnataka and Tamil Nadu?'\n",
      "q2: 'What is your say on the Andhra_Pradesh Tamils Puthiya Cauvery water dispute?'\n",
      "Replacing 3 words we have a similarity of [0.36483779]\n",
      "\n",
      "initial q1: What is the definition of linkage institution? What are some examples?\n",
      "initial q2: What is the definition of linkage institution?\n",
      "Initial similarity is [0.78038914]\n",
      "\n",
      "q1: 'What is the definition of linkage institution? What are some examples?'\n",
      "q2: 'What is the defintion of linkage institution?'\n",
      "Replacing 1 words we have a similarity of [0.52716254]\n",
      "\n",
      "q1: 'What is the definition of linkage institution? What are some examples?'\n",
      "q2: 'What is the defintion of linkages institution?'\n",
      "Replacing 2 words we have a similarity of [0.34628639]\n",
      "\n",
      "initial q1: How do I join the Mumbai Cricket Association?\n",
      "initial q2: How do I join Mumbai cricket association as a player?\n",
      "Initial similarity is [0.80852543]\n",
      "\n",
      "q1: 'How do I join the Mumbai Cricket Association?'\n",
      "q2: 'How do I joining Mumbai cricket association as a player?'\n",
      "Replacing 1 words we have a similarity of [0.58745459]\n",
      "\n",
      "q1: 'How do I join the Mumbai Cricket Association?'\n",
      "q2: 'How do I joining Kolkata cricket association as a player?'\n",
      "Replacing 2 words we have a similarity of [0.42165145]\n",
      "\n",
      "q1: 'How do I join the Mumbai Cricket Association?'\n",
      "q2: 'How do I joining Kolkata cricketing association as a player?'\n",
      "Replacing 3 words we have a similarity of [0.29269346]\n",
      "\n",
      "initial q1: How can I know for sure that I am an introvert?\n",
      "initial q2: How do I know if I am an introvert or an extrovert?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'How can I know for sure that I am an introvert?'\n",
      "q2: 'How do I tell if I am an introvert or an extrovert?'\n",
      "Replacing 1 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: Which professors at Berkeley have attracted the most research funding?\n",
      "initial q2: Which professors at Columbia have attracted the most research funding?\n",
      "Initial similarity is [0.70803868]\n",
      "\n",
      "q1: 'Which professors at Berkeley have attracted the most research funding?'\n",
      "q2: 'Which faculty at Columbia have attracted the most research funding?'\n",
      "Replacing 1 words we have a similarity of [0.52716254]\n",
      "\n",
      "q1: 'Which professors at Berkeley have attracted the most research funding?'\n",
      "q2: 'Which faculty at Columbia have attracts the most research funding?'\n",
      "Replacing 2 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: Is Acer Aspire ES1-520 suitable for an ECE student?\n",
      "initial q2: Is Acer Aspire ES1-521 suitable for an ECE student?\n",
      "Initial similarity is [0.73818471]\n",
      "\n",
      "q1: 'Is Acer Aspire ES1-520 suitable for an ECE student?'\n",
      "q2: 'Is Acer One_AO###h ES1-521 suitable for an ECE student?'\n",
      "Replacing 1 words we have a similarity of [0.48879487]\n",
      "\n",
      "q1: 'Is Acer Aspire ES1-520 suitable for an ECE student?'\n",
      "q2: 'Is Lenovo One_AO###h ES1-521 suitable for an ECE student?'\n",
      "Replacing 2 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: Can I get pregnant 14 days after my period started?\n",
      "initial q2: Can you get pregnant 8 days before your period?\n",
      "Initial similarity is [0.63267362]\n",
      "\n",
      "q1: 'Can I get pregnant 14 days after my period started?'\n",
      "q2: 'Can you getting pregnant 8 days before your period?'\n",
      "Replacing 1 words we have a similarity of [0.48026872]\n",
      "\n",
      "q1: 'Can I get pregnant 14 days after my period started?'\n",
      "q2: 'Can you getting pregnancy 8 days before your period?'\n",
      "Replacing 2 words we have a similarity of [0.3583448]\n",
      "\n",
      "initial q1: How widely accepted are credit cards at small businesses and restaurants in Barbados?\n",
      "initial q2: How widely accepted are credit cards at small businesses and restaurants in Israel?\n",
      "Initial similarity is [0.78038914]\n",
      "\n",
      "q1: 'How widely accepted are credit cards at small businesses and restaurants in Barbados?'\n",
      "q2: 'How widely accept are credit cards at small businesses and restaurants in Israel?'\n",
      "Replacing 1 words we have a similarity of [0.64226554]\n",
      "\n",
      "q1: 'How widely accepted are credit cards at small businesses and restaurants in Barbados?'\n",
      "q2: 'How widely accept are credit card at small businesses and restaurants in Israel?'\n",
      "Replacing 2 words we have a similarity of [0.52716254]\n",
      "\n",
      "q1: 'How widely accepted are credit cards at small businesses and restaurants in Barbados?'\n",
      "q2: 'How widely accept are credit card at large businesses and restaurants in Israel?'\n",
      "Replacing 3 words we have a similarity of [0.42976769]\n",
      "\n",
      "q1: 'How widely accepted are credit cards at small businesses and restaurants in Barbados?'\n",
      "q2: 'How widely accept are credit card at large busineses and restaurants in Israel?'\n",
      "Replacing 4 words we have a similarity of [0.34628639]\n",
      "\n",
      "initial q1: What are the worst secrets of Bollywood?\n",
      "initial q2: What are some dirty secrets of Bollywood?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'What are the worst secrets of Bollywood?'\n",
      "q2: 'What are some dirty secret of Bollywood?'\n",
      "Replacing 1 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: Is Islam a religion of peace or a religion of stone age?\n",
      "initial q2: Is Islam a religion of peace?\n",
      "Initial similarity is [0.66784398]\n",
      "\n",
      "q1: 'Is Islam a religion of peace or a religion of stone age?'\n",
      "q2: 'Is radical_Islam a religion of peace?'\n",
      "Replacing 1 words we have a similarity of [0.46687049]\n",
      "\n",
      "q1: 'Is Islam a religion of peace or a religion of stone age?'\n",
      "q2: 'Is radical_Islam a religions of peace?'\n",
      "Replacing 2 words we have a similarity of [0.31614037]\n",
      "\n",
      "initial q1: How is gold formed?\n",
      "initial q2: How is gold formed naturally?\n",
      "Initial similarity is [0.73818471]\n",
      "\n",
      "q1: 'How is gold formed?'\n",
      "q2: 'How is silver formed naturally?'\n",
      "Replacing 1 words we have a similarity of [0.44275367]\n",
      "\n",
      "q1: 'How is gold formed?'\n",
      "q2: 'How is silver forming naturally?'\n",
      "Replacing 2 words we have a similarity of [0.24579965]\n",
      "\n",
      "initial q1: What is the best foods and places to try and visit when visiting Iran?\n",
      "initial q2: What are the best places to visit in Iran?\n",
      "Initial similarity is [0.63267362]\n",
      "\n",
      "q1: 'What is the best foods and places to try and visit when visiting Iran?'\n",
      "q2: 'What are the finest places to visit in Iran?'\n",
      "Replacing 1 words we have a similarity of [0.48026872]\n",
      "\n",
      "q1: 'What is the best foods and places to try and visit when visiting Iran?'\n",
      "q2: 'What are the finest locales to visit in Iran?'\n",
      "Replacing 2 words we have a similarity of [0.3583448]\n",
      "\n",
      "initial q1: How do I make money in ecommerce?\n",
      "initial q2: How do ecommerce sites make money?\n",
      "Initial similarity is [0.78038914]\n",
      "\n",
      "q1: 'How do I make money in ecommerce?'\n",
      "q2: 'How do e_commerce sites make money?'\n",
      "Replacing 1 words we have a similarity of [0.52716254]\n",
      "\n",
      "q1: 'How do I make money in ecommerce?'\n",
      "q2: 'How do e_commerce sites making money?'\n",
      "Replacing 2 words we have a similarity of [0.34628639]\n",
      "\n",
      "initial q1: How do Quorans feel now that Trump is officially the next president?\n",
      "initial q2: How do you feel now that Donald Trump will be our next president?\n",
      "Initial similarity is [0.63267362]\n",
      "\n",
      "q1: 'How do Quorans feel now that Trump is officially the next president?'\n",
      "q2: 'How do you feeling now that Donald Trump will be our next president?'\n",
      "Replacing 1 words we have a similarity of [0.48026872]\n",
      "\n",
      "q1: 'How do Quorans feel now that Trump is officially the next president?'\n",
      "q2: 'How do you feeling now that Donald Donald_Trump will be our next president?'\n",
      "Replacing 2 words we have a similarity of [0.3583448]\n",
      "\n",
      "initial q1: How did you learn to speak English?\n",
      "initial q2: How I can speak English with fluency?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'How did you learn to speak English?'\n",
      "q2: 'How I can speaking English with fluency?'\n",
      "Replacing 1 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: How can I know my wife has had an affair?\n",
      "initial q2: How can I know if my wife is a cheater?\n",
      "Initial similarity is [0.6115714]\n",
      "\n",
      "q1: 'How can I know my wife has had an affair?'\n",
      "q2: 'How can I tell if my wife is a cheater?'\n",
      "Replacing 1 words we have a similarity of [0.38648109]\n",
      "\n",
      "initial q1: What is the natural way to get rid of mosquitoes?\n",
      "initial q2: What is a natural way to get rid of mosquito bites?\n",
      "Initial similarity is [0.63267362]\n",
      "\n",
      "q1: 'What is the natural way to get rid of mosquitoes?'\n",
      "q2: 'What is a Splittorff_lacked way to get rid of mosquito bites?'\n",
      "Replacing 1 words we have a similarity of [0.48026872]\n",
      "\n",
      "q1: 'What is the natural way to get rid of mosquitoes?'\n",
      "q2: 'What is a Splittorff_lacked ways to get rid of mosquito bites?'\n",
      "Replacing 2 words we have a similarity of [0.3583448]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    q1 = X_test[i, 0]\n",
    "    q2 = X_test[i, 1]\n",
    "    q1_tokenized = tp.tokenize(q1)\n",
    "    q2_tokenized = tp.tokenize(q2)\n",
    "    if model(q1_tokenized, q2_tokenized) > 0.6:\n",
    "        adversarial_white_box_change(q1, q2, model, tp, w2v)\n",
    "        print()\n",
    "#         print(i)\n",
    "#         print(q1)\n",
    "#         print(q2)\n",
    "#         print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
